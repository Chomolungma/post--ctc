<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
  integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0"
  crossorigin="anonymous">

<script type="text/front-matter">
  title: "An In-depth Guide to Connectionist Temporal Classification"
  description: "This is a guide to Connectionist Temporal Classification (CTC) – a popular algorithm for sequence to sequence mapping with neural networks."
  authors:
  - Awni Hannun : http://stanford.edu/~awni/
  affiliations:
  - Stanford University : http://cs.stanford.edu/ 
</script>

<dt-article class="centered">
  <h1>An In-depth Guide to CTC</h1>
  <dt-byline></dt-byline>

<h2 id="the-problem">The Problem</h2>

<p>Consider speech recognition. The input is a time series.  It can
have hundreds or even thousands of time-steps.
<dt-fn>
In speech it's common to use a frequency based feature extractor on the raw
audio. A spectrogram with a 10 millisecond window applied to a 10 second audio
clip yields 1000 time-steps.
</dt-fn>
The transcript is generally much
shorter, say tens of characters in length.</p>

<p>We can't train a model to predict a character for each input. We don't know
which character to use as the label. Also, not every input has a corresponding
character. There can be stretches of silence. Any rule we devise like "one
character corresponds to ten inputs" can be broken. People's rates of
speech change. Sometimes we talk quickly and sometimes slowly.</p>

<p>An alternative is to hand-align each character to it's location in the
audio. From a modeling standpoint this works well. We'd know the ground truth
for each input. But, for any reasonably sized dataset, this is prohibitively
time consuming.</p>

<figure class="l-body" style="height:250px;">
  <div style="width:330px; position:absolute;">
  <img src="assets/handwriting_recognition.svg" />
  <figcaption style="width:280px;">
  <strong>Handwriting recognition:</strong> The input can be
  <dt-math>(x,y)</dt-math> coordinates of a pen stroke or 
  pixels in an image.
  </figcaption>
  </div>
  <div style="width:280px; position:absolute; left:380px;">
  <img src="assets/speech_recognition.svg" />
  <figcaption>
  <strong>Speech recognition:</strong> The input can be a spectrogram or some
  other frequency based feature extractor.
  </figcaption>
  </div>
</figure>

<p>Connectionist Temporal Classification (CTC) solves these problems.
Certain assumptions made by the algorithm make it especially well suited to 
problems like speech recognition and handwriting recognition. Today,
it’s used in many state-of-the-art models in these domains.</p>

<p>To be a bit more formal, let's consider the problem of mapping input sequences
<dt-math>X = [x_1, x_2, \ldots, x_T] \in \mathcal{X}</dt-math> to corresponding output
sequences <dt-math>Y = [y_1, y_2, \ldots, y_U] \in \mathcal{Y}</dt-math>.
We want to learn a "good" mapping of <dt-math>X</dt-math>s to <dt-math>Y</dt-math>s.
For example, in speech recognition a "good" mapping is one which takes an audio
input to an accurate transcription.</p>

<p>The "sequence transduction" problem has three properties which make it difficult
to use traditional supervised learning algorithms.</p>
<ul>
  <li>Both <dt-math>X</dt-math> and <dt-math>Y</dt-math>
    can vary in length.</li>
  <li>The ratio of the lengths <dt-math>X</dt-math> and <dt-math>Y</dt-math> can vary.</li>
  <li>We don’t have an accurate alignment (correspondence of the elements) of
  <dt-math>X</dt-math> and <dt-math>Y</dt-math>.</li>
</ul>

<p>In these circumstances, CTC can learn a function to map elements of <dt-math>\mathcal{X}</dt-math>
to a distribution over members of <dt-math>\mathcal{Y}</dt-math>.
We can then infer a likely <dt-math>Y</dt-math> from this
distribution. For such an algorithm to be useful it needs to solve two
problems: the objective function should be computable and inference should be
tractable.</p>

<p><strong>Objective Function:</strong> We should be able to efficiently
compute a score for how likely any <dt-math>Y</dt-math> is given
an <dt-math>X</dt-math>. In our case the score will be a
conditional probability <dt-math>p(Y \mid X)</dt-math>, though this
isn’t a strict requirement.  The function <dt-math>p(Y \mid
X)</dt-math> should be differentiable. This makes optimizing the function
parameters easier.</p>

<p>We need to compute the score for <dt-math>X</dt-math>s and
<dt-math>Y</dt-math>s of variable and differing lengths. We also want to
avoid algorithms which require an alignment between <dt-math>X</dt-math>
and <dt-math>Y</dt-math>.</p>

<p><strong>Inference:</strong> Given a model for <dt-math>p(Y
\mid X)</dt-math>, for any <dt-math>X</dt-math> we need to
tractably infer a likely <dt-math>Y</dt-math>. This means solving 

<dt-math block>
Y^* = \text{argmax}_{Y \in \mathcal{Y}} p(Y \mid X).
</dt-math>

Ideally an optimal <dt-math>Y^*</dt-math> can be found
efficiently.  With CTC we’ll settle for a close to optimal solution that's not
too expensive to find.</p>

<hr />
<h2 id="the-algorithm">The Algorithm</h2>

<p>The first thing we need to do is compute a score of how likely a <dt-math>Y</dt-math>
is given an <dt-math>X</dt-math>. To
do this, the CTC model allows a set of alignments between <dt-math>X</dt-math>
and <dt-math>Y</dt-math>. To get around the fact that the alignment is unknown,
CTC <em>marginalizes</em> over all possible allowed alignments between the two
sequences. In this section we’ll cover what these allowed alignments are, how
to compute the CTC loss function and how to perform inference with a learned
model.</p>

<h3 id="alignment">Alignment</h3>
<p>The CTC algorithm assumes the alignments between the input <dt-math>X</dt-math> and the
output <dt-math>Y</dt-math> have a specific form. To motivate the CTC alignments, first
consider a naive approach.</p>

<p>Let’s use an example. Let the input length <dt-math>T = 6</dt-math>
and <dt-math>Y =</dt-math> [c, a, t]. In
this case, one way to align <dt-math>Y</dt-math> and
<dt-math>X</dt-math> would be to let the elements of
<dt-math>Y</dt-math> align to multiple elements of
<dt-math>X</dt-math>. So the alignment could be</p>

<figure style="width:300px; height:130px; margin-top:0px;">
  <figcaption>
  <span style="position:absolute; left: 0; top:15px;">output (<dt-math>Y</dt-math>)</span>
  <span style="position:absolute; left: 0; top:65px;">alignment</span>
  <span style="position:absolute; left: 0; top:110px;">input (<dt-math>X</dt-math>)</span>
  </figcaption>
  <div style="width:200px; position:absolute; left: 80px;">
  <img src="assets/naive_alignment.svg" />
  </div>
</figure>

<p>This approach has two problems.</p>
<ul>
  <li>
    Some elements of <dt-math>X</dt-math> may not
    correspond to any element of <dt-math>Y</dt-math>.
    In this case, we may not want to force every element of
    <dt-math>X</dt-math> to match with an element of
    <dt-math>Y</dt-math>.
  </li>

  <li>
    The sequence <dt-math>Y</dt-math> can have
    consecutive repeat characters. This means the alignments for a given
    <dt-math>Y</dt-math> are not unique. We have no way to
    determine if the alignment [c, c, a, a, a, t] refers to [c, a, t] or
    [c, a, a, t] among others. Later we’ll need to distinguish between these
    cases to perform inference with the model.
  </li>
</ul>

<p>To get around these problems, CTC introduces a new token to the set of allowed
output characters. This new token is sometimes called the “blank” token. We’ll
refer to it here as <dt-math>\epsilon</dt-math>. When an input
element aligns to <dt-math>\epsilon</dt-math> then no output
element corresponds to it.</p>

<p>The alignments allowed by CTC are of length <dt-math>T</dt-math>,
the length of the input. For a given <dt-math>Y</dt-math>,
let <dt-math>\mathcal{A}</dt-math>
be the set of such alignments. An alignment is in the set
<dt-math>\mathcal{A}</dt-math> if it maps to
<dt-math>Y</dt-math> as follows:</p>

<figure style="width:500px; height:240px; margin-top:0px;">
  <figcaption>
  <div style="position:absolute; top:30px; width: 150px;">
    First, merge repeat characters.
  </div>
  <div style="position:absolute; top:100px; width: 150px;">
    Then, remove any <dt-math>\epsilon</dt-math> tokens.
  </div>
  <div style="position:absolute; top:190px; width: 150px;">
    The remaining characters are the output.
  </div>
  </figcaption>
  <div style="width:400px; position:absolute; left: 100px;">
  <img src="assets/ctc_alignment_steps.svg" />
  </div>
</figure>

<p>The set <dt-math>\mathcal{A}</dt-math> contains all possible
alignments which map to <dt-math>Y</dt-math>. If there are
consecutive repeat characters in <dt-math>Y</dt-math> then
the <dt-math>\epsilon</dt-math> between them is required.
This allows us to differentiate between alignments of [c, a, t] and
[c, a, a, t].</p>

<p>Let’s go back to the [c, a, t] example with <dt-math>T =
6</dt-math>. Here are a few more examples of valid and invalid alignments.</p>

<figure style="width:500px; height:150px; margin-top:0px;">
  <figcaption>
  <div style="position:absolute; left: 13px; top:0px; width: 150px;">
    <strong>Valid Alignments</strong>
  </div>
  <div style="position:absolute; left: 233px; top:0px; width: 150px;">
    <strong>Invalid Alignments</strong>
  </div>
  <div style="position:absolute; left: 420px; top:30px; width: 100px;">
    corresponds to <dt-math>Y =</dt-math> [c, c, a, t]
  </div>
  <div style="position:absolute; left: 420px; top:90px; width: 100px;">
    has length 5
  </div>
  <div style="position:absolute; left: 420px; top:140px; width: 100px;">
    missing the 'a'
  </div>
  </figcaption>
  <div style="width:400px; position:absolute; left: 0px; top: 25px;">
  <img src="assets/valid_invalid_alignments.svg" />
  </div>
</figure>

<p>A convenient way to visualize the alignment between
<dt-math>X</dt-math> and <dt-math>Y</dt-math>
is with a 2D alignment matrix. Below is an example for the
[<dt-math>\epsilon</dt-math>, c, c,
<dt-math>\epsilon</dt-math>, a, t] alignment. Sometimes it’s
useful to include the <dt-math>\epsilon</dt-math>’s in the
alignment matrix. Both are shown here. Note that some of
the <dt-math>\epsilon</dt-math> tokens are left unaligned.
They are optional since they don’t fall between consecutive repeat characters
in <dt-math>Y</dt-math>.</p>

<figure style="height:350px;width:500px;">
  <img style="width:350px;" src="assets/alignment_no_epsilon.svg" />
  <div style="width:350px; position:absolute; top:140px;">
  <img src="assets/alignment_epsilon.svg" />
  </div>
  <figcaption style="width:130px;position:absolute; left:350px; top:30px;">
  The alignment matrix without the <dt-math>\epsilon</dt-math>
  token.
  </figcaption>
  <figcaption style="width:150px;position:absolute; left:350px; top:170px;">
  The alignment matrix including the <dt-math>\epsilon</dt-math> token.
  </figcaption>
</figure>

<p>We can observe several properties about CTC right away. First, the allowed
alignments between <dt-math>Y</dt-math> and <dt-math>X</dt-math> are strictly monotonic. If the ‘c’ in our
example aligns to the first input element then the ‘a’ must align to input
element two or greater. This implies a second property: the length of <dt-math>Y</dt-math> can
be no longer than the length of <dt-math>X</dt-math>. A third property is that the alignment
of <dt-math>X</dt-math> to <dt-math>Y</dt-math> is many-to-one. Many input elements can align to a single
output element but not vice-versa.</p>

<h3 id="loss-function">Loss Function</h3>
<p>With the alignment setup, we can write down the CTC objective
function for a single <dt-math>(X, Y)</dt-math> pair.</p>

<figure style="width:600px;height:80px;margin-top:0px;">
  <span style="position:absolute; left:30px; top:0px">
    <dt-math>p(Y \mid X)</dt-math>
  </span>
  <figcaption style="position:absolute; left:30px; top:50px; width:150px;">
  The CTC conditional <strong>probability</strong>
  </figcaption>
  <span style="position:absolute; left:170px; top:0px">
    <dt-math>=</dt-math>
  </span>
  <span style="position:absolute; left:220px; top:0px">
    <dt-math>\sum_{A \in \mathcal{A}}</dt-math>
  </span>
  <figcaption style="position:absolute; left:220px; top:50px; width:150px;">
  <strong>marginalizes</strong> over the set of valid alignments 
  </figcaption>
  <span style="position:absolute; left:420px; top:0px">
    <dt-math>\prod_{t=1}^T p_t(a_t \mid X)</dt-math>
  </span>
  <figcaption style="position:absolute; left:420px; top:50px; width:200px;">
  computing the <strong>probability</strong> for a single alignment step-by-step.
  </figcaption>
</figure>

<p>Recall <dt-math>\mathcal{A}</dt-math> is the set of alignments of <dt-math>Y</dt-math> to <dt-math>X</dt-math> allowed by
CTC. The element <dt-math>A = [a_1, \ldots, a_T]</dt-math> is a member of <dt-math>\mathcal{A}</dt-math>. The
function <dt-math>p_t(a_t \mid X)</dt-math> can be any function which produces a distribution
for each input time-step <dt-math>t</dt-math> over the output alphabet given the full
input <dt-math>X</dt-math>.</p>

<figure style="width:550px; height:350px;">
  <figcaption>
  <div style="position:absolute; top:0px; width: 200px;">
    The function <dt-math>p_t(a \mid X)</dt-math> 
    is a distribution over the alphabet
    {h, e, l, o, <dt-math>\epsilon</dt-math>} for
    each input step.
  </div>
  <div style="position:absolute; top:172px; width: 200px;">
    From the step-by-step distribution over the alphabet, we can compute the
    probability of different sequences. 
  </div>
  <div style="position:absolute; top:290px; width: 200px;">
    By marginalizaing over the alignments, we get a distribution over outputs.
  </div>
  </figcaption>
  <div style="width:300px; position:absolute; left: 230px;">
  <img src="assets/ctc_distribution.svg" />
  </div>
</figure>

<p>The hard part of the CTC loss is not understanding what it is, but computing
it efficiently. The set <dt-math>\mathcal{A}</dt-math> can be
<em>very</em> large.<dt-fn>For a <dt-math>Y</dt-math> of length
<dt-math>U</dt-math> without any repeat characters and an
<dt-math>X</dt-math> of length <dt-math>T</dt-math> the size of the set is <dt-math>{T +
U \choose T - U}</dt-math>. For <dt-math>T=100</dt-math> and
<dt-math>U=50</dt-math> this number is almost
<dt-math>10^{40}</dt-math>.</dt-fn> For most practical problems, we can’t
delineate the elements of <dt-math>\mathcal{A}</dt-math> to
compute the sum above.</p>

<p>Instead, we can compute the CTC loss both exactly and efficiently with a
dynamic programming algorithm. Like any dynamic programming algorithm, the key
is recognizing the subproblems.</p>

<p>To simplify the problem, let the sequence
<dt-math block>
Z = [z_{1}, \ldots, z_{2U+1}] = [\epsilon, y_1, \epsilon, y_2, \ldots, \epsilon, y_U, \epsilon]
</dt-math>
consist of the elements of <dt-math>Y</dt-math> with an
<dt-math>\epsilon</dt-math> at the beginning, end, and between
every character. Let <dt-math>\alpha_{s, t}</dt-math> be the
CTC score of the subsequence <dt-math>Z_{1:s}</dt-math>
after <dt-math>t</dt-math> input steps. We can compute
<dt-math>\alpha_{s, t}</dt-math> if we know the values
<dt-math>\alpha_{\lt s, \lt t}</dt-math>. There are three cases.</p>

<div style="height:320px">
<figure style="width:130px;float:right;margin-top:0px;margin-bottom:0px; margin-left:50px;">
  <img src="assets/cost_no_skip.svg" />
</figure>
<p><strong>Case 1 (<dt-math> z_{s} = \epsilon </dt-math>):</strong></p>

<p>This case says that we can't skip elements of <dt-math>Y</dt-math>
for any alignment. If we want to align the current input to
<dt-math>\epsilon</dt-math>, the previous output must be aligned to
some previous input.

<figure style="margin-top:0px;">
  <span style="position:absolute; left:40px">
    <dt-math>\alpha_{s, t} = </dt-math>
  </span>
  <span style="position:absolute; left:120px">
    <dt-math>(\alpha_{s-1, t-1} + \alpha_{s, t-1})</dt-math>
  </span>
  <figcaption style="position:absolute; left:120px; top:50px; width:240px;">
  The CTC probability of the two valid subsequences after
  <dt-math>t-1</dt-math> input steps.
  </figcaption>
  <span style="position:absolute; left:335px">
    <dt-math>\cdot</dt-math>
  </span>
  <span style="position:absolute; left:400px">
    <dt-math>p_t(z_{s} \mid X)</dt-math>
  </span>
  <figcaption style="position:absolute; left:400px; top:50px; width:200px;">
  The probability of the current character at input step <dt-math>t</dt-math>.
  </figcaption>
</figure>
</p>
</div>

<div style="height:220px;">
<figure style="width:130px;float:right;margin-top:0px;margin-bottom:0px; margin-left:50px;">
  <img src="assets/cost_repeat.svg" />
</figure>
<p><strong>Case 2 (<dt-math>z_{s} = z_{s-2}</dt-math>):</strong></p>

<p>This case handles consecutive repeats in <dt-math>Y</dt-math>.
Any alignment <em>must</em> have an <dt-math>\epsilon</dt-math>
between repeat characters. The update equation is the same as in Case 1.</p>
</div>

<div style="height:260px;">
<figure style="width:130px;float:right;margin-top:0px;margin-bottom:0px; margin-left:50px;">
  <img src="assets/cost_regular.svg" />
</figure>
<p><strong>Case 3:</strong></p>
<p>If we aren’t in the first two cases, then we’re in the third case. We have
this case because an <dt-math>\epsilon</dt-math> between unique
characters of <dt-math>Y</dt-math> is optional.</p>
<figure style="margin-top:0px;">
  <span style="position:absolute; left:40px">
    <dt-math>\alpha_{s, t} = </dt-math>
  </span>
  <span style="position:absolute; left:120px">
    <dt-math>(\alpha_{s-2, t-1} + \alpha_{s-1, t-1} + \alpha_{s, t-1})</dt-math>
  </span>
  <figcaption style="position:absolute; left:120px; top:50px; width:240px;">
  The CTC probability of the three valid subsequences after
  <dt-math>t-1</dt-math> input steps.
  </figcaption>
  <span style="position:absolute; left:395px">
    <dt-math>\cdot</dt-math>
  </span>
  <span style="position:absolute; left:420px">
    <dt-math>p_t(z_{s} \mid X)</dt-math>
  </span>
  <figcaption style="position:absolute; left:420px; top:50px; width:200px;">
  The probability of the current character at input step <dt-math>t</dt-math>.
  </figcaption>
</figure>
</p>
</div>


<p>Below is an example of the computation performed by the dynamic programming
algorithm.</p>

<figure class="l-body" style="width:700px;height:350px;margin-top:0px;">
<div style="width:650px; position:absolute; left:80px; top:30px;">
<img src="assets/ctc_cost.svg"/>
<figcaption style="width:350px;">
Node <dt-math>(s, t)</dt-math> in the diagram represents
<dt-math>\alpha_{s, t}</dt-math> – the CTC score of 
the subsequence <dt-math>Z_{1:s}</dt-math> after
<dt-math>t</dt-math> input steps.
</figcaption>
</div>
<figcaption style="position:absolute; top: 100px; width: 100px;">
<span style="position:absolute;">output</span>
<span style="position:absolute; top:25px;"><dt-math>Y =</dt-math> [a, b]</span>
</figcaption>
<figcaption style="position:absolute; left:170px; top: 0px;">
  input, <dt-math>X</dt-math>
</figcaption>
</figure>

<p>The final probability is the sum of the two output nodes
<dt-math block>
p(Y \mid X) = \alpha_{S, T} + \alpha_{S-1, T}
</dt-math>
where <dt-math>S = 2U + 1</dt-math>.</p>

<p>As long as the individual output model
<dt-math>p_t(z \mid X)</dt-math> is differentiable then
the entire loss function is differentiable. This is true since computing
<dt-math>p(Y \mid X)</dt-math> simply consists of sums and
products of the <dt-math>p_t(z \mid X)</dt-math>.</p>

<p>The time complexity of this dynamic programming algorithm is <dt-math>O(TU)</dt-math> with a
fairly small constant. Conveniently, the time to compute the loss function does
not depend at all on the size of the output alphabet.</p>

<p>For a training set <dt-math>\mathcal{D}</dt-math>, the parameters of a model are tuned to
minimize the negative log-likelihood
<dt-math block>
\sum_{(X, Y) \in \mathcal{D}} -\log p(Y \mid X)
</dt-math>
as opposed to maximizing the likelihood directly.</p>

<h3 id="inference">Inference</h3>

<p>We can find the most likely output by solving
<dt-math block>
Y^* = \text{argmax}_{Y \in \mathcal{Y}} p(Y \mid X).
</dt-math>

<p>One way to approximate this is to take the most likely output at each
time-step. This gives us the alignment with the highest probability:

<dt-math block>
A^* = \text{argmax}_{A \in \mathcal{A}} \prod_{t=1}^{T} p_t(a_t \mid X)
</dt-math>

We can then collapse repeats and remove <dt-math>\epsilon</dt-math>
tokens to produce <dt-math>Y</dt-math>.</p>

<p>For most applications this simple algorithm works well. This is because 
most of the probability mass is given to a single alignment. However, this
method is not guaranteed to find the most likely <dt-math>Y</dt-math>.
The problem is multiple alignments can map to the same <dt-math>Y</dt-math>.</p>

<p>As an example, assume the alignments [a, a, <dt-math>\epsilon</dt-math>]
and [a, a, a] individually have lower probability than [b, b, b]. However,
the sum of their probabilities is greater than that of [b, b, b]. The beam
search will incorrectly propose <dt-math>Y =</dt-math> [b] as
the most likely hypothesis. It should have chosen
<dt-math>Y =</dt-math> [a]. The algorithm needs to account for
the fact that [a, a, a] and [a, a, <dt-math>\epsilon</dt-math>]
map to the same output.</p>

<p>We can solve this problem with a modified beam search. Given limited 
computation, the modified beam search is not guaranteed to find the
most likely <dt-math>Y</dt-math>. It does, at least, have
the desirable property that we can trade-off more computation
(a larger beam-size) for an asymptotically better solution.</p>

<p>A regular beam search computes a new set of hypotheses at each input
step. The new set of hypotheses is generated from the previous set by
extending each hypothesis with all possible output characters.</p>


<figure class="l-body" style="width:700px;">
  <img src="assets/beam_search.svg" />
  <figcaption style="width:300px;">
  A standard beam search algorithm with an alphabet of
  <dt-math>\{\epsilon, a, b\}</dt-math> and a beam size
  of three.
  </figcaption>
</figure>

<p>We can modify the vanilla beam search to handle multiple alignments mapping to
the same output. In this case instead of keeping a list of alignments in the
beam, we store the output prefixes after collapsing repeats and removing
<dt-math>\epsilon</dt-math> characters. At each step of the search we accumulate scores for a
given prefix based on all the alignments which map to it. The image below
displays steps two, three and four of the algorithm. The dashed lines indicate
the output prefix that the proposed extension maps to.</p>

<figure class="l-body" style="width:900px;">
  <embed src="assets/prefix_beam_search.svg"/>
  <figcaption style="width:300px;">
  The CTC beam search algorithm with an output alphabet
  <dt-math>\{\epsilon, a, b\}</dt-math>
  and a beam size of three.
  </figcaption>
</figure>

<p>A proposed extension can map to two output prefixes if the character is a
repeat. This is shown at <dt-math>T=3</dt-math> in the figure above
where ‘a’ is proposed as an extension to the prefix [a]. Both [a] and [a, a] are
valid outputs for this proposed extension.</p>
<figure class="l-body" style="width:900px;">
  <embed src="assets/prefix_beam_search_single.svg"/>
</figure>


<p>For the [a,a] case we should only include the part of the
score of the previous prefix for alignments which end in <dt-math>\epsilon</dt-math>. This is
because <dt-math>\epsilon</dt-math> must be between consecutive repeat characters. For the [a]
case, where we don’t extend the prefix, we should only consider the part of the
score of the previous prefix for alignments which don’t end in <dt-math>\epsilon</dt-math>.</p>

<p>Given this, it’s necessary to keep track of two probabilities for each prefix
in the beam. The probability of all alignments which end in <dt-math>\epsilon</dt-math> and
the probability of all alignments which don’t end in <dt-math>\epsilon</dt-math>. Note that
when we rank the hypotheses at each step before pruning the beam, we should
rank by the combined score.</p>

<p>The implementation of this algorithm does not require much code. The code is,
however, dense and tricky to get right. Refer to this <a href="https://gist.github.com/awni/56369a90d03953e370f3964c826ed4b0">gist</a> for
an example implementation in Python.</p>

<p>In some problems, such as speech recognition, incorporating a language model
over the outputs significantly improves accuracy. To do this, we can
repose the inference problem.</p>

<figure style="width:650px;height:80px;margin-top:0px;">
  <span style="position:absolute; left:0px; top:0px">
    <dt-math>Y^* = \text{argmax}_{Y \in \mathcal{Y}}</dt-math>
  </span>
  <span style="position:absolute; left:180px; top:0px">
    <dt-math>p(Y \mid X)</dt-math>
  </span>
  <figcaption style="position:absolute; left:180px; top:50px; width:150px">
  The CTC conditional probability.
  </figcaption>
  <span style="position:absolute; left:295px; top:0px">
    <dt-math>\cdot</dt-math>
  </span>
  <span style="position:absolute; left:340px; top:0px">
    <dt-math>p(Y)^\alpha</dt-math>
  </span>
  <span style="position:absolute; left:440px; top:0px">
    <dt-math>\cdot</dt-math>
  </span>
  <figcaption style="position:absolute; left:340px; top:50px; width:150px">
  The language model probability.
  </figcaption>
  <span style="position:absolute; left:500px; top:0px">
    <dt-math>L(Y)^\beta</dt-math>
  </span>
  <figcaption style="position:absolute; left:500px; top:50px; width:100px">
  The "word" insertion bonus.
  </figcaption>
</figure>

<p>The function <dt-math>L(\cdot)</dt-math> computes the length of <dt-math>Y</dt-math> in terms of the language
model tokens and serves as a word insertion bonus. The language model scores
are only included when a prefix is extended by a character (or word) and not at
every step of the algorithm. This causes the search to favor shorter prefixes,
as measured by <dt-math>L(\cdot)</dt-math>, since they do not have many language model
updates. The insertion bonus helps with this. The parameters <dt-math>\alpha</dt-math> and
<dt-math>\beta</dt-math> are usually set by cross-validation.</p>

<hr />
<h2 id="properties-of-ctc">Properties of CTC</h2>

<p>We mentioned a few important properties of CTC so far. Here we’ll go
into more depth on what these properties are and what trade-offs they offer.</p>

<h3 id="conditional-independence">Conditional Independence</h3>

<p>One of the most commonly cited shortcomings of CTC is the conditional
independence assumption it makes. Recall, the conditional
output distribution is
<dt-math block>
p(Y \mid X) = \sum_{A \in \mathcal{A}} \prod_{t=1}^T p(a_t \mid X).
</dt-math>

<div style="height:150px;">
<figure style="width:200px;float:right;margin-top:0px;margin-bottom:0px; margin-left:50px;">
  <img src="assets/conditional_independence.svg" />
  <figcaption>
  Graphical model for CTC.
  </figcaption>
</figure>
<p>The model assumes that every output is conditionally independent of
the other outputs given the input. This is a bad assumption for many
sequence to sequence problems.</p>
</div>

<p>Say we had an audio clip of someone saying “triple A”.
<dt-cite key="Chan2016las"></dt-cite> Another valid transcription could
be “AAA”. If the first letter of the predicted transcription is ‘A’, then
the next letter should be ‘A’ with high probability and 'r' with low
probability. The conditional independence assumption does not allow for this.</p>

<p>In fact speech recognizers using CTC are not able to learn a language
model over the output nearly as well as models which do not make this
assumption.<dt-cite key="Battenberg2017"></dt-cite> However, this isn’t always
a bad trait. Baking in strong beliefs over output interactions makes the model
less adaptable to new or altered domains. For example, we might want
to use a speech recognizer trained on phone conversations between friends
for customer support calls.</p>

<h3 id="alignment-properties">Alignment Properties</h3>

<p>The CTC algorithm is <em>alignment-free</em>. The objective function marginalizes over all 
alignments. While CTC does make strong assumptions about the form of
alignments between <dt-math>X</dt-math> and
<dt-math>Y</dt-math>, the model is agnostic as to how probability
is distributed amongst
them. In some problems CTC ends up allocating most of the probability to a
single alignment. However, this isn't guaranteed. We can force the model to
choose a single alignment by replacing the sum with a max in the objective function,
<dt-math block>
p(Y \mid X) = \max_{A \in \mathcal{A}} \prod_{t=1}^T p(a_t \mid X).
</dt-math></p>

<p>As mentioned before, CTC allows only <em>strictly monotonic</em> alignments. In
problems such as speech recognition this may be a valid assumption. For other
problems like machine translation where a future word in a target sentence
can align to an earlier part of the source sentence, this assumption is a
deal-breaker. The <em>strictly monotonic</em> property also means the length of
the output can be no greater than the length of the input.
<dt-fn>
  Note that if there are <dt-math>r</dt-math> consecutive
  repeats in <dt-math>Y</dt-math>, the length
  <dt-math>U</dt-math> must be less than
  <dt-math>T</dt-math> by <dt-math>2r - 1</dt-math>.
</dt-fn>
We can't use CTC for problems where this is not the case.</p>

<p>A final important property of CTC alignments is that they are
<em>many-to-one</em>. Multiple inputs can align to at most one output. In some
cases this may not be desirable. We might want to enforce a strict
one-to-one correspondence between elements of <dt-math>X</dt-math>
and <dt-math>Y</dt-math>. Alternatively,
we may want to allow multiple output elements to align to a single input
element. For example, the sound made by “th” might align to a single input frame
of audio. A character based CTC model would not allow that.</p>

<h3 id="input-synchronous-inference">Input Synchronous Inference</h3>

<p>Inference with CTC is done in an <em>input synchronous</em> manner as opposed to an
<em>output synchronous</em> manner. This means the beam search is pruned after
processing each input element and the algorithm terminates when all of the
input has been seen. Output synchronous decoding prunes the beam after each
output time-step and typically terminates on an
end-of-sequence marker. Input synchronous inference makes streaming the
decoding process easier. For some applications streaming the inference
computation is critical to achieve low latency response times.</p>

<hr />
<h2 id="ctc-in-context">CTC in Context</h2>

<p>In this section we’ll discuss how CTC relates to other commonly used
algorithms for sequence transduction.</p>

<h3 id="hmms">HMMs</h3>
<p><em>This section requires some familiarity with the HMM and is not critical to
understanding the CTC algorithm. Feel free to skip it on a first read.</em></p>

<p>At a first glance a Hidden Markov Model (HMM) based sequence transducer does
not closely resemble a CTC model. However, the two algorithms have many
similarities. Understanding the relationship between the two models helps to
understand what exactly CTC does that couldn’t be done before. Putting CTC in
this context will also allow us to understand how it can be changed and
potentially improved for various use cases.</p>

<p>We’ll use the same notation from before, <dt-math>X</dt-math> is the input sequence and <dt-math>Y</dt-math>
is the output sequence with lengths <dt-math>T</dt-math> and <dt-math>U</dt-math> respectively. Like before
we’re interested in finding a “good” model for <dt-math>p(Y \mid X)</dt-math>. One way to
simplify the modeling problem is to transform this probability with Bayes’ Rule
and compute
<dt-math block>
p(Y \mid X) \propto p(X \mid Y) p(Y).
</dt-math>
The <dt-math>p(Y)</dt-math> term is straight-forward to model with a language model, so let’s focus
on <dt-math>p(X \mid Y)</dt-math>. Like before we’ll let <dt-math>\mathcal{A}</dt-math> be a set of allowed
of alignments of <dt-math>Y</dt-math> to <dt-math>X</dt-math>. In this case members of <dt-math>\mathcal{A}</dt-math> have length <dt-math>T</dt-math>.
Let’s otherwise leave <dt-math>\mathcal{A}</dt-math> unspecified for now. We’ll come back to
it later. We can marginalize over <dt-math>\mathcal{A}</dt-math> to get
<dt-math block>
p(X \mid Y) = \sum_{A \in \mathcal{A}} p(X, A \mid Y).
</dt-math>
To simplify notation, let’s remove the conditioning on <dt-math>Y</dt-math>, it will be
unchanging in every <dt-math>p(\cdot)</dt-math>. Using the HMM assumptions we can
write</p>

<figure style="width:650px;height:100px;">
  <figcaption>
    <div style="position:absolute; left:10px; top:50px; width:120px;">
      The probability of the <em>input</em> 
    </div>
    <div style="position:absolute; left:150px; top:50px; width:170px;">
      marginalizes over alignments
    </div>
    <div style="position:absolute; left:350px; top:50px; width:120px;">
      the emission probability 
    </div>
    <div style="position:absolute; left:500px; top:50px; width:150px;">
      and the transition probability. 
    </div>
  </figcaption>
<span style="position:absolute; left:10px;">
  <dt-math>p(X)</dt-math>
</span>
<span style="position:absolute; left:100px;">
  <dt-math>=</dt-math>
</span>
<span style="position:absolute; left:150px;">
  <dt-math>\sum_{A \in \mathcal{A}} \prod_{t=1}^T</dt-math>
</span>
<span style="position:absolute; left:350px;">
  <dt-math>p(x_t \mid a_t)</dt-math>
</span>
<span style="position:absolute; left:500px;">
  <dt-math>p(a_t \mid a_{t-1})</dt-math>
</span>

</figure>

<p>Two assumptions have been made here. The first is the usual Markov property.
The state <dt-math>a_t</dt-math> is conditionally independent of all historical states given
the previous state <dt-math>a_{t-1}</dt-math>. The second is that the observation <dt-math>x_t</dt-math> is
conditionally independent of everything else given the current state <dt-math>a_t</dt-math>.</p>

<p>Let’s assume that the transition probabilities <dt-math>p(a_t \mid a_{t-1})</dt-math> are
uniform. This gives
<dt-math block>
p(X) \propto \sum_{A \in \mathcal{A}} \prod_{t=1}^T p(x_t \mid a_t).
</dt-math>
This equation is starting to resemble the CTC loss function from above. In fact
there are only two differences. The first is that we are learning a model of
<dt-math>X</dt-math> given <dt-math>Y</dt-math> as opposed to <dt-math>Y</dt-math> given <dt-math>X</dt-math>. The second is how the set
<dt-math>\mathcal{A}</dt-math> is produced. Let’s deal with each in turn.</p>

<p>The HMM can be used with discriminative models which estimate <dt-math>p(a \mid x)</dt-math>.
To do this, we apply Bayes’ rule and rewrite the model as 
<dt-math block>
\begin{aligned}
p(X) &\propto \sum_{A \in \mathcal{A}} \prod_{t=1}^T \frac{p(a_t \mid x_t)p(x_t)}{p(a_t)} \\
&\propto \sum_{A \in \mathcal{A}} \prod_{t=1}^T \frac{p(a_t \mid x_t)}{p(a_t)}. 
\end{aligned}
</dt-math></p>

<p>If we assume a uniform prior over the states <dt-math>a</dt-math> and condition on all of
<dt-math>X</dt-math> instead of a single element at a time, we arrive at 
<dt-math block>
p(X) \propto \sum_{A \in \mathcal{A}} \prod_{t=1}^T p(a_t \mid X). 
</dt-math></p>

<p>The above equation is essentially the CTC loss function, assuming the set
<dt-math>\mathcal{A}</dt-math> is the same. In fact, the HMM framework does not specify what
<dt-math>\mathcal{A}</dt-math> should consist of. This part of the model can be designed on a
per-problem basis. In many cases the model doesn’t condition on <dt-math>Y</dt-math> and the
set <dt-math>\mathcal{A}</dt-math> consists of all possible length <dt-math>T</dt-math> sequences from the
output alphabet. In this case, the HMM can be drawn as an  <em>ergodic</em> state
transition diagram in which every state connects to every other state.  The
figure below shows this model with the alphabet or set of unique hidden states
as <dt-math>\{a, b, c\}</dt-math>.</p>

<p>In our case the hidden states of the model (the elements of <dt-math>A</dt-math>) are strongly
related to <dt-math>Y</dt-math>. We want the HMM to reflect this. One possible model could be
a simple linear state transition diagram. The figure below shows this with the
same alphabet as before and <dt-math>Y =</dt-math> [a, b]. Another commonly used model is the
<em>Bakis</em> or left-right HMM. In this model any transition which proceeds from the
left to the right is allowed.</p>

<figure class="l-body" style="width:700px;height:220px;">
  <div style="width: 150px; position: absolute;">
  <img src="assets/ergodic_hmm.svg" />
  <figcaption style="margin-left:30px;">
  <strong>Ergodic HMM:</strong> Any node can be either a starting or
  final state.
  </figcaption>
  </div>
  <div style="width: 150px; position: absolute; left: 180px;">
  <img style="margin-bottom:30px; margin-top: 30px;" src="assets/linear_hmm.svg" />
  <figcaption style="margin-left:30px;">
  <strong>Linear HMM:</strong> The first node is the starting state
  and the second node is the final state.
  </figcaption>
  </div>
  <div style="width: 350px; position: absolute; left: 360px;">
  <img style="margin-bottom:30px; margin-top: 30px;" src="assets/ctc_hmm.svg" />
  <figcaption style="margin-left:30px;width:300px;">
  <strong>CTC HMM:</strong> The first two nodes are the starting
  states and the last two nodes are the final states.
  </figcaption>
  </div>
</figure>

<p>In CTC we augment the alphabet with <dt-math>\epsilon</dt-math> and the HMM model allows a
subset of the left-right transitions. The CTC HMM has two start
states and two accepting states.</p>

<p>One possible source of confusion is that the HMM model differs for any unique
<dt-math>Y</dt-math>. This is in fact standard in applications such as speech recognition. The
state diagram changes based on the output <dt-math>Y</dt-math>. However, the functions which
estimate the observation and transition probabilities are shared.</p>

<p>Let’s discuss how CTC improves on the original HMM model. First, we can think
of the CTC state diagram as a special case HMM which works well for many
problems of interest. Incorporating the blank as a hidden state in the HMM
allows us to use the alphabet of <dt-math>Y</dt-math> as the other hidden states. This model
also gives a set of allowed alignments which may be a good prior for some
problems. Perhaps most importantly, CTC is discriminative as it models <dt-math>p(Y \mid X)</dt-math> directly. This allows us to train the model “end-to-end” and
unleashes the capacity of powerful models like the RNN.</p>

<h3 id="encoder-decoder-models">Encoder-Decoder Models</h3>

<p>The neural encoder-decoder is perhaps the most commonly used framework for
sequence transduction. This class of models consists of an encoder and a
decoder. The encoder maps the input sequence <dt-math>X</dt-math> into a hidden
representation. The decoder consumes the hidden representation and produces a
distribution over the output space <dt-math>\mathcal{Y}</dt-math>. We can write this as

<dt-math block>
\begin{aligned}
h &= \texttt{encode}(X) \\
p(Y \mid X) &= \texttt{decode}(h).
\end{aligned}
</dt-math>

The <dt-math>\texttt{encode}(\cdot)</dt-math> and <dt-math>\texttt{decode}(\cdot)</dt-math> functions are typically RNNs.
The decoder can optionally be equipped with an attention mechanism. The hidden
state <dt-math>h</dt-math> usually has dimensions
<dt-math>T \times d</dt-math> where <dt-math>d</dt-math> is
a hyperparameter. Sometimes the encoder subsamples the input. If the encoder
subsamples the input by a factor <dt-math>s</dt-math> then the
inner dimension of <dt-math>h</dt-math> will be <dt-math>\frac{T}{s}</dt-math>.</p>

<p>We can interpret CTC in the encoder-decoder framework. This is helpful to
understand the developments in encoder-decoder models that are applicable to
CTC. Also, it’s useful to develop a common language for the properties of these
models.</p>

<p><strong>Encoder:</strong> The encoder of a CTC model can be just about any encoder we find
in commonly used encoder-decoder models. For example the encoder could be a
multi-layer bidirectional RNN or a convolutional network. There is a constraint
on the CTC encoder that doesn’t apply to the others. The input length cannot be
subsampled so much that <dt-math>\frac{T}{s}</dt-math> is less than <dt-math>U</dt-math>.</p>

<p><strong>Decoder:</strong> We can view the decoder of a CTC model as a simple linear
transformation followed by a softmax normalization. This layer should project
all <dt-math>T</dt-math> components of the encoder output <dt-math>h</dt-math> into the dimensionality of the
output alphabet.</p>

<hr />
<h2 id="practitioners-guide">Practitioner’s Guide</h2>

<p>So far we’ve mostly developed a conceptual understanding of CTC. Here we’ll go
through a few implementation tips for practitioners.</p>

<p><strong>Software:</strong> Even with a solid conceptual understanding of CTC, the
implementation is difficult. The algorithm has several edge cases and a fast
implementation needs to be written in a lower-level programming language.
Open-source software tools can make getting started with CTC much easier:</p>

<ul>
  <li>Baidu Research has open-sourced <a href="https://github.com/baidu-research/warp-ctc">warp-ctc</a>. The package is written in C++ and
CUDA. The CTC loss function runs on either the CPU or the GPU. Bindings are
available for Torch, TensorFlow and <a href="https://github.com/awni/warp-ctc">PyTorch</a>.</li>
  <li>TensorFlow has built in <a href="https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss">CTC loss</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder">CTC beam search</a> functions. The TensorFlow CTC implementation doesn't support the GPU yet.</li>
  <li>Nvidia also provides a GPU implementation of CTC in <a href="https://developer.nvidia.com/cudnn">cuDNN</a> versions 7 and up.</li>
</ul>

<p><strong>Numerical Stability:</strong> Computing the CTC loss naively is numerically
instable.  One method to avoid this is to normalize the <dt-math>\alpha</dt-math>’s at each
time-step.  The <a href="ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf">original publication</a> has more detail on this
including the adjustments to the gradient. In practice this works well enough
for medium length sequences but can still underflow for long sequences.
A better solution is to compute the loss function in log-space with the
log-sum-exp trick
<dt-fn>
When computing the sum of two probabilities in log space use the identity
<dt-math block>
\log(e^a + e^b) = \max\{a, b\} + \log(1 + e^{-|a-b|})
</dt-math>
Most programming languages have a stable function to compute
<dt-math>\log(1 + x)</dt-math> when
<dt-math>x</dt-math> is close to zero.
</dt-fn>.

Inference should also be done in log-space using the log-sum-exp trick.</p>

<p><strong>Beam Search:</strong> There are a couple of good tips to know about when
implementing  and using the CTC beam search.</p>

<p>The correctness of the beam search can be tested as follows.</p>
<ol>
  <li>Run the beam search algorithm on an arbitrary input.</li>
  <li>Save the inferred output <dt-math>\bar{Y}</dt-math> and the corresponding score <dt-math>\bar{c}</dt-math>.</li>
  <li>Compute the actual CTC score <dt-math>c</dt-math> for <dt-math>\bar{Y}</dt-math> using the same input.</li>
  <li>Check that <dt-math>\bar{c} \approx c</dt-math> with the former being no greater than the later.
As the beam size increases the inferred output <dt-math>\bar{Y}</dt-math> may change, but the two
numbers should grow closer.</li>
</ol>

<p>A common question when using a beam search decoder is the size of the beam to
use.  There is a trade-off between accuracy and runtime. We can check if the
beam size is in a good range. To do this first compute the CTC score for the
inferred output <dt-math>c_i</dt-math>. Then compute the CTC score for the ground truth output
<dt-math>c_g</dt-math>. If the two outputs are not the same, we should have <dt-math>c_g \lt c_i</dt-math>.
If <dt-math>c_i << c_g</dt-math> then the beam search is performing poorly and a large
increase in the beam size may be warranted.</p>

<hr />
<h2 class="no_toc" id="bibliographic-notes">Bibliographic Notes</h2>

<p>The CTC algorithm was first published by Graves et al. in 2006.
<dt-cite key="Graves2006"></dt-cite> The first experiments were on TIMIT,
a popular phoneme recognition benchmark.<dt-cite key="Lopes2011"></dt-cite>
Chapter 7 of Graves’ thesis<dt-cite key="Graves2012"></dt-cite> gives a
more detailed treatment of CTC.</p>

<p>One of the first applications of CTC to large vocabulary speech recognition was
by Graves et al. in 2014 where they used a hybrid HMM and CTC
trained model to achieve state-of-the-art results.<dt-cite key="Graves2014"></dt-cite>
Hannun et al. subsequently demonstrated state-of-the-art CTC based speech
recognition on larger benchmarks.<dt-cite key="Hannun2014deepspeech"></dt-cite>
State-of-the-art results on an online handwriting recognition task using the CTC algorithm
with an RNN were achieved in 2007.<dt-cite key="Liwicki2007"></dt-cite></p>

<p>The CTC algorithm has been used successfully in other problems including lip
reading from video<dt-cite key="Assael2016"></dt-cite>, action labelling from
video<dt-cite key="Huang2016"></dt-cite> and keyword detection in audio.
<dt-cite key="Fernandez2007,Lengerich2016"></dt-cite></p>

<p>Many extensions and improvements to CTC have been proposed. Here are a few.
The <em>Sequence Transducer</em> discards the conditional independence assumption
made by CTC.<dt-cite key="Graves2012transducer"></dt-cite> As a consequence the
model also allows the output to be longer than the input. The <em>Gram-CTC</em> model
generalizes CTC to marginalize over n-gram output classes.
<dt-cite key="Liu2017"></dt-cite> Other works have generalized CTC or proposed
similar algorithms to account for segmental structure in the output.
<dt-cite key="Wang2017,Kong2016"></dt-cite></p>

<p>The Hidden Markov Model was developed in the 1960’s with the first application
to speech recognition in the 1970’s. For an introduction to the HMM and
applications to speech recognition see Rabiner’s canonical tutorial.
<dt-cite key="Rabiner1989"></dt-cite></p>

<p>Encoder-decoder models were developed in 2014.
<dt-cite key="Cho2014,Sutskever2014"></dt-cite> The online publication
<em>Distill</em> gives an in-depth guide to attention in encoder-decoder
models.<dt-cite key="Olah2016"></dt-cite></p>

</dt-article>

<dt-appendix class="centered">
</dt-appendix>

<script type="text/bibliography">
@article{Battenberg2017,
archivePrefix = {arXiv},
arxivId = {1707.07413},
author = {Battenberg, Eric and Chen, Jitong and Child, Rewon and Coates, Adam and Gaur, Yashesh and Li, Yi and Liu, Hairong and Satheesh, Sanjeev and Seetapun, David and Sriram, Anuroop and Zhu, Zhenyao},
eprint = {1707.07413},
month = {jul},
title = {Exploring Neural Transducers for End-to-End Speech Recognition},
url = {http://arxiv.org/abs/1707.07413},
year = {2017}
}
@article{Olah2016,
author = {Olah, C and Carter, S},
journal = {Distill},
title = {Attention and augmented recurrent neural networks},
url = {http://distill.pub/2016/augmented-rnns/},
year = {2016}
}
@article{Sutskever2014,
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
journal = {Advances in neural information processing systems},
month = {sep},
title = {Sequence to Sequence Learning with Neural Networks},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}
@article{Cho2014,
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
isbn = {9781937284961},
issn = {09205691},
journal = {EMNLP},
pmid = {2079951},
title = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
url = {https://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Rabiner1989,
author = {Rabiner, L.R. R.},
doi = {10.1109/5.18626},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {2},
pages = {p257--286},
title = {Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
url = {http://ieeexplore.ieee.org/abstract/document/18626/},
volume = {77},
year = {1989}
}
@inproceedings{Liwicki2007,
author = {Liwicki, Marcus and Graves, Alex and Bunke, Horst and Schmidhuber, Jürgen},
booktitle = {Proceedings - 9th Int. Conf. on Document Analysis and Recognition},
doi = {10.1.1.139.5852},
pages = {367--371},
title = {A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks},
url = {https://www.cs.toronto.edu/~graves/icdar_2007.pdf},
volume = {1},
year = {2007}
}
@book{Graves2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v1},
author = {Graves, Alex},
booktitle = {Springer},
doi = {10.1007/978-3-642-24797-2},
eprint = {arXiv:1308.0850v1},
isbn = {978-3-642-24796-5},
issn = {01406736},
pmid = {7491034},
title = {Supervised Sequence Labelling with Recurrent Neural Networks},
url = {http://link.springer.com/10.1007/978-3-642-24797-2},
volume = {385},
year = {2012}
}
@article{Lopes2011,
author = {Lopes, Carla and Perdigão, Fernando},
doi = {10.5772/17600},
isbn = {978-953-307-996-7},
issn = {9789533070865},
journal = {Speech Technologies},
pages = {285--302},
title = {Phone recognition on the TIMIT database},
url = {https://www.intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database/},
volume = {1},
year = {2011}
}
@article{Graves2014,
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Graves, Alex and Jaitly, Navdeep},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
month = {jan},
number = {1},
pages = {1764--1772},
pmid = {1000285842},
title = {Towards End-To-End Speech Recognition with Recurrent Neural Networks},
url = {http://jmlr.org/proceedings/papers/v32/graves14.pdf},
volume = {32},
year = {2014}
}
@article{Graves2006,
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
pmid = {1000285842},
title = {Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
url = {ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf},
year = {2006}
}
@inproceedings{Chan2016las,
author = {Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
booktitle = {ICASSP},
title = {Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition},
url = {https://arxiv.org/abs/1508.01211},
year = {2016}
}
@article{Hannun2014deepspeech,
author = {Hannun, Awni Y and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y},
title = {Deep Speech: Scaling up end-to-end speech recognition},
url = {http://arxiv.org/abs/1412.5567},
volume = {abs/1412.5},
year = {2014}
}
@article{Huang2016,
archivePrefix = {arXiv},
arxivId = {1607.08584},
author = {Huang, De-An and Fei-Fei, Li and Niebles, Juan Carlos},
doi = {10.1007/978-3-319-46493-0},
eprint = {1607.08584},
isbn = {9783319464930},
issn = {0302-9743},
journal = {European Conference on Computer Vision},
month = {jul},
pages = {137----153},
pmid = {10463930},
title = {Connectionist Temporal Modeling for Weakly Supervised Action Labeling},
url = {http://arxiv.org/abs/1607.08584},
year = {2016}
}
@article{Assael2016,
archivePrefix = {arXiv},
arxivId = {1611.01599},
author = {Assael, Yannis M. and Shillingford, Brendan and Whiteson, Shimon and de Freitas, Nando},
eprint = {1611.01599},
month = {nov},
title = {LipNet: End-to-End Sentence-level Lipreading},
url = {http://arxiv.org/abs/1611.01599},
year = {2016}
}
@article{Lengerich2016,
archivePrefix = {arXiv},
arxivId = {1611.09405},
author = {Lengerich, Chris and Hannun, Awni},
eprint = {1611.09405},
journal = {NIPS 2016 End-to-End Learning for Speech and Audio Processing Workshop},
month = {nov},
title = {An End-to-End Architecture for Keyword Spotting and Voice Activity Detection},
url = {http://arxiv.org/abs/1611.09405},
year = {2016}
}
@article{Fernandez2007,
author = {Fernández, Santiago and Graves, Alex and Schmidhuber, Jürgen},
doi = {10.1007/978-3-540-74695-9_23},
isbn = {9783540746935},
issn = {03029743},
journal = {The 17th international conference on Artificial neural networks},
pages = {220--229},
title = {An application of recurrent neural networks to discriminative keyword spotting},
url = {http://link.springer.com/10.1007/978-3-540-74695-9_23},
year = {2007}
}
@article{Graves2012transducer,
archivePrefix = {arXiv},
arxivId = {1211.3711},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {1211.3711},
isbn = {2000201075},
issn = {18792782},
pmid = {23459267},
title = {Sequence Transduction with Recurrent Neural Networks},
url = {https://arxiv.org/pdf/1211.3711.pdf http://arxiv.org/abs/1211.3711},
year = {2012}
}
@article{Liu2017,
archivePrefix = {arXiv},
arxivId = {1703.00096},
author = {Liu, Hairong and Zhu, Zhenyao and Li, Xiangang and Satheesh, Sanjeev},
eprint = {1703.00096},
journal = {Proceedings of the 34th International Conference on Machine Learning},
month = {feb},
title = {Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling},
url = {http://arxiv.org/abs/1703.00096},
year = {2017}
}
@article{Wang2017,
archivePrefix = {arXiv},
arxivId = {1702.07463},
author = {Wang, Chong and Wang, Yining and Huang, Po-Sen and Mohamed, Abdelrahman and Zhou, Dengyong and Deng, Li},
eprint = {1702.07463},
month = {feb},
title = {Sequence Modeling via Segmentations},
url = {http://arxiv.org/abs/1702.07463},
year = {2017}
}
@article{Kong2016,
archivePrefix = {arXiv},
arxivId = {1511.06018},
author = {Kong, Lingpeng and Dyer, Chris and Smith, Noah A.},
doi = {10.21437/Interspeech.2016-40},
eprint = {1511.06018},
journal = {ICLR},
month = {nov},
title = {Segmental Recurrent Neural Networks},
url = {http://arxiv.org/abs/1511.06018},
year = {2016}
}
</script>
